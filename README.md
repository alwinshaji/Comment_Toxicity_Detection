# Comment Toxicity Detection

This project is a machine learning application designed to detect and classify toxic comments. It leverages advanced deep learning models like **LSTM**, **BiLSTM**, **CNN**, and **BiGRU** to analyze and predict comment toxicity levels based on text input. The project includes a Streamlit web app where users can interactively test the models and see predictions in real-time.  

ðŸ”— **Live Demo**: [Comment Toxicity Detection](https://commenttoxicitydetection.streamlit.app)

---

## Features
- Built with multiple deep learning models (LSTM, BiLSTM, CNN, BiGRU) for performance comparison
- Extensive preprocessing including tokenization, padding, and cleaning of text data
- Trained and evaluated on a labeled dataset of toxic comments
- Deployed via Streamlit for easy and interactive use
- Model-ready tokenizer file included for seamless predictions

---

## Tech Stack
- **Python**
- **TensorFlow/Keras**
- **Streamlit**
- **Pandas**, **NumPy**, **Matplotlib**, **Seaborn**
- **NLTK / text preprocessing libraries**

---

## Project Structure


â”œâ”€â”€ app.py # Streamlit app code

â”œâ”€â”€ comment-toxicity-detection.ipynb # Notebook for training & experimentation

â”œâ”€â”€ requirements.txt # Dependencies

â”œâ”€â”€ tokenizer.pkl # Tokenizer file for text preprocessing

â””â”€â”€ README.md # Project documentation

---

## Future Improvements

- Fine-tuning models for better accuracy

- Adding ensemble models

- Improving UI/UX of the web application

- Expanding dataset for broader toxicity categories

---





















